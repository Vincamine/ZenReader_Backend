import requests
import os
from util.GenerateQuery import generate_llm_query
from util.ResponseDealer import process_llm_response

from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# LLM endpoint configuration
LLM_API_ENDPOINT = os.environ.get('LLM_API_ENDPOINT', 'http://localhost:65268/api/v1/workspace/hackathon/chat')
LLM_API_KEY = os.environ.get('LLM_API_KEY', '')

def send_to_llm(query):
    """
    Sends a query to the LLM endpoint and returns the response.

    Args:
        query (dict): The query generated by generate_llm_query()

    Returns:
        dict: The response from the LLM
    """
    headers = {
        'Content-Type': 'application/json'
    }

    if LLM_API_KEY:
        headers['Authorization'] = f'Bearer {LLM_API_KEY}'

    try:
        response = requests.post(
            LLM_API_ENDPOINT,
            json=query,
            headers=headers,
            timeout=30
        )

        if response.status_code != 200:
            print(f"LLM API error: {response.status_code} - {response.text}")
            return None

        return response.json()

    except Exception as e:
        print(f"Error sending request to LLM: {e}")
        return None


def break_text_into_chunks(text):
    """
    Breaks the input text into smaller chunks by sentence punctuation marks.

    Args:
        text (str): The text to split into chunks

    Returns:
        list: List of text chunks (sentences)
    """
    # Define punctuation marks to split by (period, question mark, exclamation mark)
    punctuation_marks = ['.', '?', '!', '。', '？', '！']

    chunks = []
    current_chunk = ""

    # Process text character by character
    i = 0
    while i < len(text):
        current_chunk += text[i]

        # Check if current character is a punctuation mark
        if text[i] in punctuation_marks and current_chunk.strip():
            chunks.append(current_chunk.strip())
            current_chunk = ""

        i += 1

    # Add the last chunk if there's any remaining text
    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks


def process_text(text):
    """
    End-to-end function to take input text, send to LLM, and return processed HTML.

    Args:
        text (str): The text to process

    Returns:
        str: HTML string from processed LLM responses
    """
    # Break text into smaller chunks
    chunks = break_text_into_chunks(text)

    # Collect processed HTML paragraphs for each chunk
    processed_chunks_html = []

    for chunk in chunks:
        # Generate query for LLM
        query = generate_llm_query(chunk)

        # Send query to LLM and get response
        llm_raw_response = send_to_llm(query)

        # Process the response and return formatted HTML for the chunk
        if llm_raw_response:
            processed_html = process_llm_response(llm_raw_response, chunk)
            # Optional: wrap each processed chunk in its own div (for styling)
            processed_chunks_html.append(f"<div class='chunk'>{processed_html}</div>")
        else:
            processed_chunks_html.append(f"<div class='chunk'><p>Error processing chunk: {chunk}</p></div>")

    # Combine all processed chunks inside the outer container
    html_output = "<div class='adhd-reader-content'>" + "".join(processed_chunks_html) + "</div>"

    return html_output
